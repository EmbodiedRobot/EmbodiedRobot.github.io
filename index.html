<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>JD Explore Academy – VLA Lab</title>
    <style>
        /* CSS样式：采用专业化设计，使用Flexbox和Grid布局，确保响应式；背景统一浅灰，避免蓝色调，前景字体黑色优化 */
        body {
            font-family: 'Arial', sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f5f5f5; /* 浅灰背景，类似于VLA模型中的背景噪声抑制，提升视觉和谐 */
            color: #000000; /* 前景字体统一黑色，高对比度可读性 */
        }
        header {
            background-color: #f5f5f5; /* 同步浅灰背景，避免蓝色 */
            color: #000000; /* 字体黑色 */
            padding: 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            border-bottom: 1px solid #ddd; /* 添加轻边框以区分，类似于边缘检测 */
        }
        .logo {
            font-size: 24px;
            font-weight: bold;
        }
        nav ul {
            list-style: none;
            display: flex;
            gap: 20px;
        }
        nav a {
            color: #000000; /* 导航链接黑色 */
            text-decoration: none;
            font-weight: bold;
        }
        main {
            max-width: 1200px;
            margin: 40px auto;
            padding: 0 20px;
        }
        .blog-post {
            background-color: white;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 30px;
            display: grid;
            grid-template-columns: 1fr 3fr; /* 图像与文本网格布局 */
            gap: 20px;
        }
        .blog-post img {
            width: 100%;
            height: auto;
            border-radius: 8px;
        }
        .post-content h2 {
            margin-top: 0;
        }
        .post-meta {
            color: #666; /* 元数据稍浅灰，但整体前景黑基调 */
            font-size: 14px;
        }
        footer {
            background-color: #f5f5f5; /* 底部同步浅灰背景，避免蓝色 */
            color: #000000; /* 字体黑色 */
            text-align: center;
            padding: 20px;
            margin-top: 40px;
            border-top: 1px solid #ddd; /* 添加轻边框以区分 */
        }
        footer a {
            color: #000000; /* 链接黑色 */
        }
        /* 响应式设计：媒体查询，类似于深度学习中的自适应层 */
        @media (max-width: 768px) {
            .blog-post {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="logo">JD Explore Academy – VLA Lab</div>
        <nav>
            <ul>
                <li><a href="./index.html">Home</a></li>
                <!-- <li><a href="/blog">Blog</a></li>
                <li><a href="/join-us">Join Us</a></li>
                <li><a href="/contact">Contact</a></li> -->
            </ul>
        </nav>
    </header>
    
    <main>
        <h1>Blog</h1>
        <p>We are bringing general Embodied AI into the physical world.</p>
        
        <!-- Blog posts list: Updated based on retrieved data, akin to sequence generation and multimodal fusion in large language models -->
        <div class="blog-post">
            <img src="https://embodiedrobot.github.io/static/image/overview.jpg" alt="BRMData Dataset">
            <div class="post-content">
                <h2><a href="./BRMData.html">BRMData: A Bimanual-Mobile Robot Manipulation Dataset for Household Tasks</a></h2>
                <div class="post-meta">2024</div>
                <p>BRMData is a Bimanual-mobile Robot Manipulation Dataset designed for household applications, featuring 10 diverse tasks including single-arm and dual-arm, tabletop, and mobile manipulations. It includes multi-view and depth-sensing data, with tasks ranging from single-object to multi-object grasping, non-interactive to human-robot interactive scenarios, and rigid to flexible-object manipulation. A novel Manipulation Efficiency Score (MES) metric is introduced to evaluate the precision and efficiency of robot manipulation methods.</p>
            </div>
        </div>
        
        <div class="blog-post">
            <img src="https://yihanghku.github.io/OFA/asserts/overview.jpg" alt="Object-Focus Actor">
            <div class="post-content">
                <h2><a href="https://yihanghku.github.io/OFA/">Object-Focus Actor for Data-efficient Robot Generalization Dexterous Manipulation</a></h2>
                <div class="post-meta">2025</div>
                <p>The Object-Focus Actor (OFA) introduces a data-efficient approach for generalized dexterous robot manipulation, addressing limitations in generalization across diverse scenes and object placements. It employs a hierarchical pipeline including object perception and pose estimation, pre-manipulation pose arrival, and object-focus policy learning, leveraging consistent end trajectories for efficient policy training. Real-world experiments across seven tasks demonstrate superior performance in positional and background generalization, achieving robust results with only 10 demonstrations.</p>
            </div>
        </div>
        
        <div class="blog-post">
            <img src="https://via.placeholder.com/300x200?text=JDVLA-Align" alt="JDVLA-Align Representation Learning">
            <div class="post-content">
                <h2>JDVLA-Align: Enhancing Visual Language Action Representation Learning</h2>
                <div class="post-meta">August 12, 2025</div>
                <p>JDVLA-Align enhances visual-language-action (VLA) representation learning by leveraging pre-trained CLIP models for aligning vision and language, extracting task-relevant visual representations to improve 3D spatial understanding in VLA models. This approach supports robot generalization based on language instructions through early fusion mechanisms for representation augmentation.</p>
            </div>
        </div>
        
        <div class="blog-post">
            <img src="https://via.placeholder.com/300x200?text=JDVLA-RL" alt="JDVLA-RL Manipulation">
            <div class="post-content">
                <h2>JDVLA-RL: Towards General Robotic Manipulation with Reinforcement Learning</h2>
                <div class="post-meta">May 24, 2025</div>
                <p>JDVLA-RL is an algorithmic framework that utilizes online reinforcement learning (RL) to refine pre-trained autoregressive VLA models, enabling scalable general robotic manipulation. It improves task completion alignment and dynamic skill acquisition through RL fine-tuning.</p>
            </div>
        </div>
        
        <div class="blog-post">
            <img src="https://via.placeholder.com/300x200?text=JDVLA-Human" alt="JDVLA-Human Learning">
            <div class="post-content">
                <h2>JDVLA-Human: Learning from Human Video</h2>
                <div class="post-meta">July 16, 2025</div>
                <p>JDVLA-Human trains VLA models from egocentric human videos, predicting wrist and hand actions which are then converted to robot motions via inverse kinematics and retargeting. This supports large-scale learning from human videos with dynamic pose estimation.</p>
            </div>
        </div>
    </main>

    <!-- <footer>
        <p>© 2025 Physical Intelligence. All rights reserved. | <a href="/privacy">Privacy Policy</a> | <a href="/terms">Terms of Service</a></p>
        <p>Contact: research@physicalintelligence.company</p>
    </footer> -->
</body>
</html>